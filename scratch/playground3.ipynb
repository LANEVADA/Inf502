{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(in_features, rank, bias=False)\n",
    "        self.up = nn.Linear(rank, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(self.down(x))\n",
    "\n",
    "class ImageTextToVideoGenerator(nn.Module):\n",
    "    def __init__(self, image_size=32, text_size=512, latent_dim=128, rank=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.text_size = text_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_size = image_size * image_size * 3 + text_size\n",
    "\n",
    "        # Two LSTMs for forward and backward motion\n",
    "        self.lstm_next = nn.LSTM(input_size=self.input_size, hidden_size=self.latent_dim, batch_first=True)\n",
    "        self.lstm_prev = nn.LSTM(input_size=self.input_size, hidden_size=self.latent_dim, batch_first=True)\n",
    "\n",
    "        # LoRA layers for frame generation\n",
    "        self.lora_fc1 = LoRALayer(self.latent_dim, 512, rank=rank)\n",
    "        self.lora_fc2 = LoRALayer(512, 256, rank=rank)\n",
    "        self.lora_fc3 = LoRALayer(256, 128, rank=rank)\n",
    "\n",
    "        # Output layers for next/previous frames\n",
    "        self.fc_out = nn.Linear(128, 3 * image_size * image_size)\n",
    "\n",
    "    def forward(self, image_features, text_features, hidden_next=None, hidden_prev=None):\n",
    "        batch_size = image_features.size(0)\n",
    "        image_features = image_features.view(batch_size, -1)  \n",
    "        text_features = text_features.view(batch_size, -1)  \n",
    "        \n",
    "        combined_features = torch.cat([image_features, text_features], dim=1).unsqueeze(1)\n",
    "        noise = torch.randn_like(combined_features) * 0.1\n",
    "        combined_features = combined_features + noise\n",
    "        # Generate next frame\n",
    "        lstm_out_next, hidden_next = self.lstm_next(combined_features, hidden_next)\n",
    "        lstm_out_next = lstm_out_next.squeeze(1)\n",
    "        x_next = F.relu(self.lora_fc1(lstm_out_next))\n",
    "        x_next = F.relu(self.lora_fc2(x_next))\n",
    "        x_next = F.relu(self.lora_fc3(x_next))\n",
    "        output_frame_next = self.fc_out(x_next).view(batch_size, 3, self.image_size, self.image_size)\n",
    "\n",
    "        # Generate previous frame\n",
    "        lstm_out_prev, hidden_prev = self.lstm_prev(combined_features, hidden_prev)\n",
    "        lstm_out_prev = lstm_out_prev.squeeze(1)\n",
    "        x_prev = F.relu(self.lora_fc1(lstm_out_prev))\n",
    "        x_prev = F.relu(self.lora_fc2(x_prev))\n",
    "        x_prev = F.relu(self.lora_fc3(x_prev))\n",
    "        output_frame_prev = self.fc_out(x_prev).view(batch_size, 3, self.image_size, self.image_size)\n",
    "\n",
    "        return output_frame_prev, output_frame_next, hidden_prev, hidden_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import clip\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# CIFAR-10 Dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Step 2: Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Step 3: Random text feature generator\n",
    "text_descriptions = [\n",
    "    \"A cat jumping over a fence.\",\n",
    "    \"A dog playing with a ball in the park.\",\n",
    "    \"A person sitting at a desk working on a laptop.\",\n",
    "    \"A car driving down a road during sunset.\",\n",
    "    \"A group of people walking down the street.\",\n",
    "    \"A child playing with a toy in the living room.\",\n",
    "    \"A beautiful mountain landscape during sunrise.\",\n",
    "    \"A person cooking food in the kitchen.\",\n",
    "    \"A group of birds flying in the sky.\"\n",
    "]\n",
    "\n",
    "def generate_random_text_feature(batch_size):\n",
    "    random_descriptions = np.random.choice(text_descriptions, size=batch_size)\n",
    "    text_input = clip.tokenize(random_descriptions).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    return text_features, random_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, dataloader, num_epochs=50, device=\"cuda\"):\n",
    "    generator.to(device)\n",
    "    optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Generate random text features for each batch\n",
    "            text_features, _ = generate_random_text_feature(images.size(0))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Initialize hidden states\n",
    "            hidden_next = None\n",
    "            hidden_prev = None\n",
    "\n",
    "            # Forward pass through the generator\n",
    "            prev_frame, next_frame, hidden_prev, hidden_next = generator(images, text_features, hidden_next, hidden_prev)\n",
    "            recon_frame, _, _,_ = generator(prev_frame, text_features, None, hidden_prev)\n",
    "            _,recon_frame2, _, _ = generator(next_frame, text_features, hidden_next, None)\n",
    "            # Loss calculation using cycle consistency\n",
    "            loss = F.mse_loss(recon_frame, images)  # Enforce cycle consistency\n",
    "            loss += F.mse_loss(recon_frame2, images)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            torch.save(generator.state_dict(), f\"image_text_to_video_epoch{epoch+1}.pth\")\n",
    "            print(f\"Model saved! Epoch: {epoch+1}\")\n",
    "    # Save the trained model\n",
    "    torch.save(generator.state_dict(), \"image_text_to_video.pth\")\n",
    "    print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.06837911158800125\n",
      "Epoch [2/50], Loss: 0.063211590051651\n",
      "Epoch [3/50], Loss: 0.06870967894792557\n",
      "Epoch [4/50], Loss: 0.06174229457974434\n",
      "Epoch [5/50], Loss: 0.05803699791431427\n",
      "Epoch [6/50], Loss: 0.07680699229240417\n",
      "Epoch [7/50], Loss: 0.07444904744625092\n",
      "Epoch [8/50], Loss: 0.06532055884599686\n",
      "Epoch [9/50], Loss: 0.07597209513187408\n",
      "Epoch [10/50], Loss: 0.05918559432029724\n",
      "Model saved! Epoch: 10\n",
      "Epoch [11/50], Loss: 0.06262199580669403\n",
      "Epoch [12/50], Loss: 0.06586286425590515\n",
      "Epoch [13/50], Loss: 0.051472101360559464\n",
      "Epoch [14/50], Loss: 0.06625089049339294\n",
      "Epoch [15/50], Loss: 0.060234081000089645\n",
      "Epoch [16/50], Loss: 0.06675697863101959\n",
      "Epoch [17/50], Loss: 0.06832742691040039\n",
      "Epoch [18/50], Loss: 0.06786681711673737\n",
      "Epoch [19/50], Loss: 0.068850576877594\n",
      "Epoch [20/50], Loss: 0.07954797893762589\n",
      "Model saved! Epoch: 20\n",
      "Epoch [21/50], Loss: 0.06120537221431732\n",
      "Epoch [22/50], Loss: 0.06305098533630371\n",
      "Epoch [23/50], Loss: 0.0741385966539383\n",
      "Epoch [24/50], Loss: 0.06605672091245651\n",
      "Epoch [25/50], Loss: 0.065575510263443\n",
      "Epoch [26/50], Loss: 0.05531885847449303\n",
      "Epoch [27/50], Loss: 0.04901869595050812\n"
     ]
    }
   ],
   "source": [
    "generator = ImageTextToVideoGenerator()  # Use your model here\n",
    "train(generator, train_dataloader, num_epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A group of birds flying in the sky.']\n",
      "Video saved as outputs/output.avi!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import clip\n",
    "# Load the trained model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "generator = ImageTextToVideoGenerator()  # Your model class\n",
    "generator.load_state_dict(torch.load(\"image_text_to_video_epoch20.pth\"))\n",
    "generator.to(device)\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    preprocess_transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = preprocess_transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image_tensor.to(device)\n",
    "# Function to generate a video from the model\n",
    "def generate_video(generator, initial_image, text_embedding, num_frames=20, filename=\"outputs/output.avi\"):\n",
    "    generator.eval()  # Set model to evaluation mode\n",
    "    initial_image = initial_image.to(device).unsqueeze(0)  # Add batch dimension\n",
    "    text_embedding = text_embedding.to(device).unsqueeze(0)\n",
    "\n",
    "    # Initialize hidden states for previous and next frames\n",
    "    hidden_next = None\n",
    "    hidden_prev = None\n",
    "    \n",
    "    frames = []\n",
    "\n",
    "    # Start with the initial image and generate frames\n",
    "    current_image = initial_image\n",
    "    original_image = initial_image.squeeze(0).squeeze(0)  # Remove batch dimension\n",
    "    original_image_np = original_image.permute(1, 2, 0).cpu().numpy()  # Shape (3, 32, 32) -> (32, 32, 3)\n",
    "    original_image_np = (original_image_np * 255).astype(np.uint8)  # Convert to [0, 255] range\n",
    "    frames.append(original_image_np)\n",
    "    for _ in range(num_frames):\n",
    "        # Generate previous and next frames from the current image and text embedding\n",
    "        prev_frame, next_frame, hidden_prev, hidden_next = generator(current_image, text_embedding, hidden_next, hidden_prev)\n",
    "\n",
    "        # Use the next frame as the new input for the next iteration\n",
    "        current_image = next_frame\n",
    "        \n",
    "        # Convert the generated frame to a numpy array and append to the frames list\n",
    "        frame_np = next_frame.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()  # Shape (3, 32, 32) -> (32, 32, 3)\n",
    "        frame_np = (frame_np * 255).astype(np.uint8)  # Convert to [0, 255] range\n",
    "        frames.append(frame_np)\n",
    "\n",
    "    # Save the frames as a video using OpenCV\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")  # Codec for video writing\n",
    "    out = cv2.VideoWriter(filename, fourcc, 10, (width, height))  # 10 FPS\n",
    "\n",
    "    for frame in frames:\n",
    "        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # Convert RGB to BGR\n",
    "\n",
    "    out.release()  # Finalize the video file\n",
    "    print(f\"Video saved as {filename}!\")\n",
    "\n",
    "# Example to generate a video\n",
    "sample_image_path = \"images/test.jpg\"  # Path to your image\n",
    "sample_image = load_and_preprocess_image(sample_image_path)\n",
    "sample_text, real_text = generate_random_text_feature(1)  # Generate random text feature\n",
    "print(real_text)\n",
    "generate_video(generator, sample_image, sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
