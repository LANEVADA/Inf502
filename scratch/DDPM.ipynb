{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shu/.conda/envs/inf502/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define 3D convolutional UNet architecture for video generation\n",
    "class VideoDDPM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoDDPM, self).__init__()\n",
    "        # Define your 3D UNet here (you can replace this with any suitable architecture)\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Use 3D convolutions for spatial-temporal feature extraction\n",
    "            nn.Conv3d(3, 64, kernel_size=3, stride=1, padding=1),  # Input: (batch_size, 3, T, H, W)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),  # Downsampling in time and space\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # Add more layers as necessary\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # To output normalized images in the range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Custom dataset class for video frames\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_folder, frame_count=3, transform=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.frame_count = frame_count\n",
    "        self.transform = transform\n",
    "        self.video_files = sorted(os.listdir(video_folder))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files) - self.frame_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = []\n",
    "        for i in range(self.frame_count):\n",
    "            frame_path = os.path.join(self.video_folder, self.video_files[idx + i])\n",
    "            frame = cv2.imread(frame_path)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        frames = torch.stack(frames)  # Stack frames into a tensor (T, H, W, C)\n",
    "        return frames\n",
    "\n",
    "# Training loop\n",
    "def train_ddpm(model, dataset, epochs=100, batch_size=8, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Define DDPM noise schedule, loss function, and other necessary components\n",
    "    # For simplicity, here we use a basic MSE loss between predicted frames and ground truth frames\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, frames in enumerate(tqdm(data_loader)):\n",
    "            frames = frames.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Add noise to frames for DDPM\n",
    "            noisy_frames, noise = add_noise(frames)\n",
    "\n",
    "            # Forward pass through DDPM model\n",
    "            predicted_frames = model(noisy_frames)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = mse_loss(predicted_frames, frames)  # Example: MSE between predicted and true frames\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss / len(data_loader)}\")\n",
    "\n",
    "# Noise addition function (this is a very simplified version)\n",
    "def add_noise(frames, noise_factor=0.1):\n",
    "    noise = torch.randn_like(frames) * noise_factor\n",
    "    noisy_frames = frames + noise\n",
    "    return noisy_frames, noise\n",
    "\n",
    "# Dataset and training setup\n",
    "transform = T.Compose([T.Resize((64, 64)), T.ToTensor()])\n",
    "video_dataset = VideoDataset(video_folder=\"path/to/video/frames\", frame_count=16, transform=transform)\n",
    "\n",
    "model = VideoDDPM()\n",
    "\n",
    "# Train the DDPM model\n",
    "train_ddpm(model, video_dataset, epochs=100, batch_size=8, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
