{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import clip  # For text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text_descriptions = [\n",
    "    \"A cat jumping over a fence.\",\n",
    "    \"A dog playing with a ball in the park.\",\n",
    "    \"A person sitting at a desk working on a laptop.\",\n",
    "    \"A car driving down a road during sunset.\",\n",
    "    \"A group of people walking down the street.\",\n",
    "    \"A child playing with a toy in the living room.\",\n",
    "    \"A beautiful mountain landscape during sunrise.\",\n",
    "    \"A person cooking food in the kitchen.\",\n",
    "    \"A group of birds flying in the sky.\"\n",
    "]\n",
    "\n",
    "# Initialize CLIP model and preprocess\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Random text feature generator (choose one text description randomly)\n",
    "def generate_random_text_feature(batch_size):\n",
    "    \"\"\"\n",
    "    Generate random text features for the given batch size.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): The number of random text features to generate.\n",
    "        text_size (int): The size of each text feature (e.g., embedding size).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Random text features of shape (batch_size, text_size).\n",
    "    \"\"\"\n",
    "    # Randomly choose a description for each image in the batch\n",
    "    random_descriptions = np.random.choice(text_descriptions, size=batch_size)\n",
    "    \n",
    "    # Tokenize and encode text descriptions using CLIP\n",
    "    text_input = clip.tokenize(random_descriptions).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_input)  # Shape: (batch_size, text_size)\n",
    "    \n",
    "    return text_features,random_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        \n",
    "        # Original weight matrix (frozen)\n",
    "        self.W_0 = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        # Low-rank matrices A and B\n",
    "        self.A = nn.Parameter(torch.randn(out_features, rank))\n",
    "        self.B = nn.Parameter(torch.randn(rank, in_features))\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_normal_(self.W_0)\n",
    "        nn.init.kaiming_normal_(self.A)\n",
    "        nn.init.kaiming_normal_(self.B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform the LoRA adjustment\n",
    "        weight = self.W_0 + torch.matmul(self.A, self.B)\n",
    "        return F.linear(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextToVideoGenerator(nn.Module):\n",
    "    def __init__(self, image_size=32, text_size=512, rank=4):\n",
    "        super(ImageTextToVideoGenerator, self).__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.text_size = text_size\n",
    "        \n",
    "        # Total input size should be image_size * image_size * 3 (for RGB) + text_size\n",
    "        self.input_size = image_size * image_size * 3 + text_size\n",
    "\n",
    "        # LoRA layers for predicting next and previous frames\n",
    "        self.lora_fc1_next = LoRALayer(self.input_size, 512,rank=rank)\n",
    "        self.lora_fc2_next = LoRALayer(512, 256,rank=rank)\n",
    "        self.lora_fc3_next = LoRALayer(256, 128,rank=rank)\n",
    "        \n",
    "        self.lora_fc1_prev = LoRALayer(self.input_size, 512,rank=rank)\n",
    "        self.lora_fc2_prev = LoRALayer(512, 256,rank=rank)\n",
    "        self.lora_fc3_prev = LoRALayer(256, 128,rank=rank)\n",
    "\n",
    "        # Final output layers for next and previous frame predictions\n",
    "        self.fc_out_next = nn.Linear(128, 3 * image_size * image_size)  # Output image (next frame)\n",
    "        self.fc_out_prev = nn.Linear(128, 3 * image_size * image_size)  # Output image (previous frame)\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "    # Flatten the image features\n",
    "        image_features = image_features.view(image_features.size(0), -1)  # Flatten to (batch_size, 3*32*32)\n",
    "\n",
    "        # Make sure text_features is a 2D tensor\n",
    "        text_features = text_features.view(text_features.size(0), -1)  # Ensure it's (batch_size, text_size)\n",
    "        # Combine image and text features\n",
    "        combined_features = torch.cat([image_features, text_features], dim=1)  # Shape: (batch_size, 3200)\n",
    "\n",
    "        # First layer for next frame\n",
    "        x_next = F.relu(self.lora_fc1_next(combined_features))  # Shape: (batch_size, 512)\n",
    "        x_next = F.relu(self.lora_fc2_next(x_next))  # Shape: (batch_size, 256)\n",
    "        x_next = F.relu(self.lora_fc3_next(x_next))  # Shape: (batch_size, 128)\n",
    "\n",
    "        # First layer for previous frame\n",
    "        x_prev = F.relu(self.lora_fc1_prev(combined_features))  # Shape: (batch_size, 512)\n",
    "        x_prev = F.relu(self.lora_fc2_prev(x_prev))  # Shape: (batch_size, 256)\n",
    "        x_prev = F.relu(self.lora_fc3_prev(x_prev))  # Shape: (batch_size, 128)\n",
    "\n",
    "        # Output video frames (flattened)\n",
    "        output_frame_prev = self.fc_out_prev(x_prev)  # Shape: (batch_size, 3 * 32 * 32)\n",
    "        output_frame_prev = output_frame_prev.view(-1, 3, self.image_size, self.image_size)  # Reshape to (batch_size, 3, 32, 32)\n",
    "\n",
    "        output_frame_next = self.fc_out_next(x_next)  # Shape: (batch_size, 3 * 32 * 32)\n",
    "        output_frame_next = output_frame_next.view(-1, 3, self.image_size, self.image_size)  # Reshape to (batch_size, 3, 32, 32)\n",
    "\n",
    "        return output_frame_prev, output_frame_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8849543142570415\n",
      "Epoch [2/10], Loss: 0.5083020650188814\n",
      "Epoch [3/10], Loss: 0.4722380752908215\n",
      "Epoch [4/10], Loss: 0.4501917041690397\n",
      "Epoch [5/10], Loss: 0.4406351975271966\n",
      "Epoch [6/10], Loss: 72.88154170515823\n",
      "Epoch [7/10], Loss: 0.5392954900946589\n",
      "Epoch [8/10], Loss: 0.5387999266290695\n",
      "Epoch [9/10], Loss: 0.537877062155662\n",
      "Epoch [10/10], Loss: 0.5359363305172093\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, epochs=10, lr=1e-3, device='cuda'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (images, label) in enumerate(dataloader):\n",
    "            images= images.to(device)\n",
    "            text_features,_ = generate_random_text_feature(images.size(0)).to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: predict previous and next frames\n",
    "            pred_prev, pred_next = model(images, text_features)\n",
    "            _, prev_next = model(pred_prev, text_features)\n",
    "            next_prev, _ = model(pred_next, text_features)\n",
    "            # Calculate loss between predicted frames and the real frames\n",
    "            loss = F.mse_loss(next_prev, images) + F.mse_loss(prev_next, images)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader)}\")\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize the generator model\n",
    "model = ImageTextToVideoGenerator().to(device)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Initialize CIFAR-10 dataset and dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the model and move to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train(model, train_dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def save_video(frames, filename=\"generated_video.mp4\", fps=30):\n",
    "    \"\"\"\n",
    "    Save a sequence of frames as a video.\n",
    "    \n",
    "    Args:\n",
    "        frames (list): List of frames (numpy arrays) to be saved as video.\n",
    "        filename (str): The output video filename.\n",
    "        fps (int): Frames per second in the video.\n",
    "    \"\"\"\n",
    "    height, width, _ = frames[0].shape  # Get frame size\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in frames:\n",
    "        # Ensure frame is uint8 (0-255) before writing\n",
    "        frame = np.clip(frame, 0, 255).astype(np.uint8)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def generate_video(model, dataloader, device, num_frames=10):\n",
    "    \"\"\"\n",
    "    Generate a video by iteratively creating multiple frames starting from a single image.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model to generate frames.\n",
    "        dataloader: DataLoader with input images and text features.\n",
    "        device: Device to run the model (e.g., \"cuda\" or \"cpu\").\n",
    "        num_frames (int): Number of frames to generate for the video.\n",
    "    \n",
    "    Returns:\n",
    "        List of generated frames.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    frames = []\n",
    "    \n",
    "    # Get a single image from the dataloader\n",
    "    images, _ = next(iter(dataloader))\n",
    "    plt.imshow(((images[0]+1)/2).permute(1, 2, 0))\n",
    "    plt.axis(\"off\")  # Hide axis for better visualization\n",
    "    plt.show()\n",
    "    image = images[0].unsqueeze(0).to(device)  # Take the first image and keep batch dim\n",
    "\n",
    "    # Generate a random text feature for this video\n",
    "    text_features, text_description = generate_random_text_feature(1)  # 1 text feature\n",
    "    print(\"Text Description:\", text_description)\n",
    "    \n",
    "    # Start with the original image as the first frame\n",
    "    frames.append(image.cpu().numpy().squeeze().transpose(1, 2, 0))  # Convert to (H, W, C)\n",
    "\n",
    "    # Iteratively generate frames\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_frames - 1):\n",
    "            # Generate next frame using the model\n",
    "            generated_frame, _ = model(image.view(image.size(0), -1), text_features.to(device))\n",
    "            \n",
    "            # Prepare next frame for generation\n",
    "            image = generated_frame.view(1, 3, 32, 32)  # Reshape if needed (adjust based on your image size)\n",
    "            \n",
    "            # Convert generated frame to numpy for visualization\n",
    "            generated_frame_np = generated_frame.cpu().numpy().squeeze().transpose(1, 2, 0)  # (C, H, W) â†’ (H, W, C)\n",
    "            generated_frame_np = (generated_frame_np + 1) / 2\n",
    "            # Ensure the pixel values are in range [0, 255] and dtype is uint8\n",
    "            generated_frame_np = np.clip(generated_frame_np * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "            \n",
    "            frames.append(generated_frame_np)\n",
    "\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Description: ['A cat jumping over a fence.']\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Generate a video with 10 frames using the dataloader\n",
    "frames = generate_video(model, train_dataloader, device, num_frames=80)\n",
    "\n",
    "# Save the generated frames as a video\n",
    "save_video(frames, \"outputs/generated_video.mp4\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
